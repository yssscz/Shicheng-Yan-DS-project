{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM6K8nq9S4m9MCvAIBlf28k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yssscz/Shicheng-Yan-DS-project/blob/main/ds_5220.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# load data\n",
        "train_data = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
        "test_data = pd.read_csv(\"test.tsv\", sep=\"\\t\")\n",
        "\n",
        "# check\n",
        "print(train_data.head())\n",
        "\n",
        "\n",
        "# get the text and label\n",
        "train_texts = train_data[\"Phrase\"].tolist()\n",
        "train_labels = train_data[\"Sentiment\"].tolist()\n",
        "\n",
        "# split dataset\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# test dataset\n",
        "test_texts = test_data[\"Phrase\"].tolist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQBPN-S5gaS6",
        "outputId": "5f2e9de9-e2ab-4bf7-efee-dcfa832a8b0a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PhraseId  SentenceId                                             Phrase  \\\n",
            "0         1           1  A series of escapades demonstrating the adage ...   \n",
            "1         2           1  A series of escapades demonstrating the adage ...   \n",
            "2         3           1                                           A series   \n",
            "3         4           1                                                  A   \n",
            "4         5           1                                             series   \n",
            "\n",
            "   Sentiment  \n",
            "0          1  \n",
            "1          2  \n",
            "2          2  \n",
            "3          2  \n",
            "4          2  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data['Sentiment'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqwEOyTZVhqd",
        "outputId": "e350b59b-7247-4d0a-d396-eb667f6ff695"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment\n",
            "2    79582\n",
            "3    32927\n",
            "1    27273\n",
            "4     9206\n",
            "0     7072\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    # remove special characters\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "F5m2b94qXbVO"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  # remove stopwords\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return \" \".join(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1c19KD_ZS3n",
        "outputId": "3327e2e1-b5fe-4884-e339-44b0a5394fc1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_pipeline(text):\n",
        "    text = remove_special_characters(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "0nQZNVmKco_3"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[\"Cleaned_Phrase\"] = train_data[\"Phrase\"].apply(clean_text_pipeline)\n",
        "\n",
        "print(train_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-7F2yRjc6t_",
        "outputId": "6f7f0989-c41a-4454-d130-2c7cc32fd3d8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PhraseId  SentenceId                                             Phrase  \\\n",
            "0         1           1  A series of escapades demonstrating the adage ...   \n",
            "1         2           1  A series of escapades demonstrating the adage ...   \n",
            "2         3           1                                           A series   \n",
            "3         4           1                                                  A   \n",
            "4         5           1                                             series   \n",
            "\n",
            "   Sentiment                                     Cleaned_Phrase  \n",
            "0          1  A series escapades demonstrating adage good go...  \n",
            "1          2  A series escapades demonstrating adage good goose  \n",
            "2          2                                           A series  \n",
            "3          2                                                  A  \n",
            "4          2                                             series  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicates_and_empty(data): # clean dataset\n",
        "\n",
        "    data = data[data['Cleaned_Phrase'].str.strip() != \"\"]\n",
        "\n",
        "    data = data.drop_duplicates(subset=['Cleaned_Phrase'])\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "NhRj8bafefeo"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_single_word_and_sentiment(data):\n",
        "# Delete all rows where the Cleaned_Phrase column contains only one word and the Sentiment value is 2.\n",
        "    data['Cleaned_Phrase'] = data['Cleaned_Phrase'].fillna(\"\")\n",
        "\n",
        "    data = data[~((data['Cleaned_Phrase'].str.split().str.len() <= 2) & (data['Sentiment'] == 2))]\n",
        "\n",
        "    return data\n",
        "cleaned_train_data2 = clean_single_word_and_sentiment(cleaned_train_data)"
      ],
      "metadata": {
        "id": "bnqiWSWPglTl"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cleaned_train_data2['Sentiment'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTvODIWEg0eT",
        "outputId": "872c25c8-6f0f-4eb8-dd06-78455b6cf31c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment\n",
            "2    19436\n",
            "3    18901\n",
            "1    15953\n",
            "4     5331\n",
            "0     4139\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# cleaned dataset split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    cleaned_train_data2['Cleaned_Phrase'],\n",
        "    cleaned_train_data2['Sentiment'],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "Ue0NGYbWicE5"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import torch\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "bert_model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "\n",
        "# Data preprocessing function\n",
        "def preprocess_function(texts, labels=None):\n",
        "    encodings = tokenizer(list(texts), padding=\"max_length\", truncation=True, max_length=256)\n",
        "    if labels is not None:\n",
        "        encodings[\"labels\"] = torch.tensor(list(labels))\n",
        "    return encodings\n",
        "\n",
        "# Using the cleaned dataset\n",
        "train_encodings = preprocess_function(X_train, y_train)\n",
        "val_encodings = preprocess_function(X_val, y_val)\n",
        "\n",
        "# Load the BERT model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(bert_model_name, num_labels=5)\n",
        "\n",
        "# Define Trainer arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert_results\",\n",
        "    overwrite_output_dir=True,\n",
        "    run_name=\"bert_finetune_experiment\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,  # Reduce batch size to save GPU memory\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    logging_dir=\"./bert_logs\",\n",
        ")\n",
        "\n",
        "# Define custom dataset\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        return item\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings)\n",
        "val_dataset = CustomDataset(val_encodings)\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Validation set predictions\n",
        "predictions = trainer.predict(val_dataset)\n",
        "preds = predictions.predictions.argmax(-1)  # Get predicted classes\n",
        "\n",
        "# Compute evaluation metrics\n",
        "print(\"Accuracy:\", accuracy_score(y_val, preds))\n",
        "print(\n",
        "    classification_report(\n",
        "        y_val,\n",
        "        preds,\n",
        "        target_names=[\"Negative\", \"Somewhat Negative\", \"Neutral\", \"Somewhat Positive\", \"Positive\"]\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "R7T-4u_KuKT-",
        "outputId": "0f881d92-57c2-4114-c40c-7fd3426386b4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-46-31773be43387>:55: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "<ipython-input-46-31773be43387>:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9564' max='9564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9564/9564 10:06, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.927100</td>\n",
              "      <td>0.930744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.760400</td>\n",
              "      <td>0.889718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.641500</td>\n",
              "      <td>0.947049</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-31773be43387>:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-46-31773be43387>:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-46-31773be43387>:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-46-31773be43387>:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6393506900878294\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "         Negative       0.53      0.44      0.48       813\n",
            "Somewhat Negative       0.63      0.66      0.65      3207\n",
            "          Neutral       0.66      0.65      0.65      3877\n",
            "Somewhat Positive       0.66      0.69      0.67      3768\n",
            "         Positive       0.59      0.51      0.55      1087\n",
            "\n",
            "         accuracy                           0.64     12752\n",
            "        macro avg       0.61      0.59      0.60     12752\n",
            "     weighted avg       0.64      0.64      0.64     12752\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import torch\n",
        "\n",
        "# Load the GPT2 tokenizer and model\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
        "\n",
        "# set Padding Token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Data preprocessing function\n",
        "def preprocess_function(texts, labels=None, max_len=128):\n",
        "    encodings = tokenizer(\n",
        "        list(texts),\n",
        "        max_length=max_len,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    if labels is not None:\n",
        "        encodings[\"labels\"] = torch.tensor(list(labels), dtype=torch.long)\n",
        "    return encodings\n",
        "\n",
        "# using the cleaned dataset\n",
        "train_encodings = preprocess_function(X_train, y_train, max_len=128)\n",
        "val_encodings = preprocess_function(X_val, y_val, max_len=128)\n",
        "\n",
        "# Define custom dataset\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        return item\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings)\n",
        "val_dataset = CustomDataset(val_encodings)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "# Define Trainer arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./gpt2_logs\",\n",
        "    logging_steps=10,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "results = trainer.evaluate()\n",
        "print(\"Evaluation results:\", results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "kgIalLoA1Fo_",
        "outputId": "a5fa1e3b-59ce-4a7e-c42a-95eed647ad62"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-47-ae375e46581d>:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9564' max='9564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9564/9564 09:02, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.041300</td>\n",
              "      <td>0.985746</td>\n",
              "      <td>0.583516</td>\n",
              "      <td>0.582656</td>\n",
              "      <td>0.583516</td>\n",
              "      <td>0.577694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.869300</td>\n",
              "      <td>0.941025</td>\n",
              "      <td>0.604297</td>\n",
              "      <td>0.603373</td>\n",
              "      <td>0.604297</td>\n",
              "      <td>0.595553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.793900</td>\n",
              "      <td>0.938718</td>\n",
              "      <td>0.611355</td>\n",
              "      <td>0.609831</td>\n",
              "      <td>0.611355</td>\n",
              "      <td>0.607541</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='797' max='797' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [797/797 00:12]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.9387183785438538, 'eval_accuracy': 0.6113550815558344, 'eval_precision': 0.6098305746861503, 'eval_recall': 0.6113550815558344, 'eval_f1': 0.6075411118287439, 'eval_runtime': 12.6017, 'eval_samples_per_second': 1011.931, 'eval_steps_per_second': 63.246, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Predict on the validation set using the trained model\n",
        "predictions = trainer.predict(val_dataset)\n",
        "preds = np.argmax(predictions.predictions, axis=-1)  # Get predicted classes\n",
        "\n",
        "# Print the shape of the predictions (to check alignment)\n",
        "print(\"Predictions shape:\", preds.shape)\n",
        "\n",
        "# Print the shape of the validation labels\n",
        "print(\"Validation labels shape:\", np.array(y_val).shape)\n",
        "\n",
        "# Ensure predictions and true labels are aligned\n",
        "if len(preds) < len(y_val):\n",
        "    y_val = y_val[:len(preds)]\n",
        "elif len(preds) > len(y_val):\n",
        "    preds = preds[:len(y_val)]\n",
        "\n",
        "# Output classification report\n",
        "report = classification_report(\n",
        "    y_val, preds, target_names=[\"Negative\", \"Somewhat Negative\", \"Neutral\", \"Somewhat Positive\", \"Positive\"]\n",
        ")\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "# Output overall accuracy\n",
        "accuracy = accuracy_score(y_val, preds)\n",
        "print(\"Overall Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "OAFV84dL6Ixy",
        "outputId": "492f67a9-0c3d-4f04-ce31-d80fefa3d56d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions shape: (12752,)\n",
            "Validation labels shape: (12752,)\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "         Negative       0.54      0.34      0.42       813\n",
            "Somewhat Negative       0.59      0.65      0.62      3207\n",
            "          Neutral       0.65      0.61      0.63      3877\n",
            "Somewhat Positive       0.61      0.68      0.65      3768\n",
            "         Positive       0.57      0.45      0.50      1087\n",
            "\n",
            "         accuracy                           0.61     12752\n",
            "        macro avg       0.59      0.55      0.56     12752\n",
            "     weighted avg       0.61      0.61      0.61     12752\n",
            "\n",
            "Overall Accuracy: 0.6113550815558344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import torch\n",
        "# Load the BERT tokenizer\n",
        "bert_model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "\n",
        "# Load original dataset\n",
        "train_data = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
        "\n",
        "# Get the text and label\n",
        "train_texts = train_data[\"Phrase\"].tolist()\n",
        "train_labels = train_data[\"Sentiment\"].tolist()\n",
        "\n",
        "# Split dataset\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Data preprocessing function\n",
        "def preprocess_function(texts, labels=None):\n",
        "    encodings = tokenizer(list(texts), padding=\"max_length\", truncation=True, max_length=256)\n",
        "    if labels is not None:\n",
        "        encodings[\"labels\"] = torch.tensor(list(labels), dtype=torch.long)\n",
        "    return encodings\n",
        "\n",
        "# Using the original dataset split\n",
        "train_encodings = preprocess_function(train_texts, train_labels)\n",
        "val_encodings = preprocess_function(val_texts, val_labels)\n",
        "\n",
        "# Load the BERT model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(bert_model_name, num_labels=5)\n",
        "\n",
        "# Define Trainer arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert_results\",\n",
        "    overwrite_output_dir=True,\n",
        "    run_name=\"bert_finetune_experiment\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    logging_dir=\"./bert_logs\",\n",
        ")\n",
        "\n",
        "# Define custom dataset\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            key: val[idx].clone().detach() if isinstance(val[idx], torch.Tensor) else torch.tensor(val[idx])\n",
        "            for key, val in self.encodings.items()\n",
        "        }\n",
        "        return item\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings)\n",
        "val_dataset = CustomDataset(val_encodings)\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Validation set predictions\n",
        "predictions = trainer.predict(val_dataset)\n",
        "preds = predictions.predictions.argmax(-1)  # Get predicted classes\n",
        "\n",
        "# Ensure consistency in label lengths\n",
        "y_val = val_encodings[\"labels\"].tolist()\n",
        "assert len(y_val) == len(preds), \"y_val and preds have inconsistent lengths!\"\n",
        "\n",
        "# Compute evaluation metrics\n",
        "print(\"Accuracy:\", accuracy_score(y_val, preds))\n",
        "print(\n",
        "    classification_report(\n",
        "        y_val,\n",
        "        preds,\n",
        "        target_names=[\"Negative\", \"Somewhat Negative\", \"Neutral\", \"Somewhat Positive\", \"Positive\"]\n",
        "    )\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "24Mwf46Gjr1p",
        "outputId": "c893d746-1add-4b9d-b924-86fd1693c76d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-3-bf5ad20f3871>:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myscz\u001b[0m (\u001b[33myscz-northeastern-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241206_063146-8rey1xd1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/yscz-northeastern-university/huggingface/runs/8rey1xd1' target=\"_blank\">bert_finetune_experiment</a></strong> to <a href='https://wandb.ai/yscz-northeastern-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/yscz-northeastern-university/huggingface' target=\"_blank\">https://wandb.ai/yscz-northeastern-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/yscz-northeastern-university/huggingface/runs/8rey1xd1' target=\"_blank\">https://wandb.ai/yscz-northeastern-university/huggingface/runs/8rey1xd1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='23409' max='23409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [23409/23409 22:47, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.717717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.642000</td>\n",
              "      <td>0.709056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.551500</td>\n",
              "      <td>0.763388</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6980328078943996\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "         Negative       0.52      0.55      0.54      1416\n",
            "Somewhat Negative       0.60      0.61      0.61      5527\n",
            "          Neutral       0.80      0.79      0.79     15639\n",
            "Somewhat Positive       0.64      0.60      0.62      6707\n",
            "         Positive       0.55      0.62      0.59      1923\n",
            "\n",
            "         accuracy                           0.70     31212\n",
            "        macro avg       0.62      0.64      0.63     31212\n",
            "     weighted avg       0.70      0.70      0.70     31212\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "import torch\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "def preprocess_function(texts, labels=None, max_len=128):\n",
        "    encodings = tokenizer(\n",
        "        list(texts),\n",
        "        max_length=max_len,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    if labels is not None:\n",
        "        encodings[\"labels\"] = torch.tensor(list(labels), dtype=torch.long)\n",
        "    return encodings\n",
        "\n",
        "train_encodings = preprocess_function(train_texts, train_labels, max_len=128)\n",
        "val_encodings = preprocess_function(val_texts, val_labels, max_len=128)\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        return item\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings)\n",
        "val_dataset = CustomDataset(val_encodings)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./gpt2_logs\",\n",
        "    logging_steps=10,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "results = trainer.evaluate()\n",
        "print(\"Evaluation results:\", results)\n",
        "\n",
        "predictions = trainer.predict(val_dataset)\n",
        "preds = torch.argmax(torch.tensor(predictions.predictions), dim=-1)\n",
        "report = classification_report(\n",
        "    val_labels, preds.numpy(), target_names=[\"Negative\", \"Somewhat Negative\", \"Neutral\", \"Somewhat Positive\", \"Positive\"]\n",
        ")\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "lL9P1VoY6rNW",
        "outputId": "1dd89d5f-744e-4203-96f0-bd16075bb48d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-5-8a3bbeed182b>:70: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='18729' max='18729' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [18729/18729 16:30, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.692000</td>\n",
              "      <td>0.797276</td>\n",
              "      <td>0.668442</td>\n",
              "      <td>0.666830</td>\n",
              "      <td>0.668442</td>\n",
              "      <td>0.666695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.632000</td>\n",
              "      <td>0.759405</td>\n",
              "      <td>0.687946</td>\n",
              "      <td>0.686939</td>\n",
              "      <td>0.687946</td>\n",
              "      <td>0.686594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.610800</td>\n",
              "      <td>0.767990</td>\n",
              "      <td>0.687465</td>\n",
              "      <td>0.687378</td>\n",
              "      <td>0.687465</td>\n",
              "      <td>0.687031</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.7594050168991089, 'eval_accuracy': 0.6879455346415699, 'eval_precision': 0.6869389108374753, 'eval_recall': 0.6879455346415699, 'eval_f1': 0.6865943839947385, 'eval_runtime': 23.5278, 'eval_samples_per_second': 1061.298, 'eval_steps_per_second': 66.347, 'epoch': 3.0}\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "         Negative       0.55      0.40      0.47      1093\n",
            "Somewhat Negative       0.58      0.62      0.60      4413\n",
            "          Neutral       0.78      0.79      0.78     12648\n",
            "Somewhat Positive       0.61      0.62      0.61      5324\n",
            "         Positive       0.59      0.51      0.55      1492\n",
            "\n",
            "         accuracy                           0.69     24970\n",
            "        macro avg       0.62      0.59      0.60     24970\n",
            "     weighted avg       0.69      0.69      0.69     24970\n",
            "\n"
          ]
        }
      ]
    }
  ]
}